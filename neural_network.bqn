âŸ¨DualâŸ© â† â€¢Import "dual.bqn"
âŸ¨GnuPlotâŸ© â† â€¢Import "BQN-Gnuplot/Gnuplot.bqn"
rng â† â€¢MakeRand 0

lr â† 0.01

# Single layer neural network with weights, biases and a sigmoid activation function
Network â† {insz ğ•Š outsz:
    âŸ¨insz, outszâŸ© â‡

    real_weights â† outszâ€¿insz rng.Range 0
    real_biases â† outsz rng.Range 0

    n_params â‡ (outszÃ—insz) + outsz

    z â‡ 0 Dual n_paramsâ¥Š0

    weights â‡ (â†•â‰¢real_weights) {oâ€¿i ğ•Š w:
        idx â† (o Ã— insz) + i
        eps â† 1âŒ¾(idxâŠ¸âŠ‘) n_paramsâ¥Š0
        w Dual eps
    }Â¨real_weights

    biases â‡ (â†•â‰¢real_biases) {o ğ•Š w:
        idx â† (outsz Ã— insz) + o
        eps â† 1âŒ¾(idxâŠ¸âŠ‘) n_paramsâ¥Š0
        w Dual eps
    }Â¨real_biases

    Forward â‡ {
        x â† ğ•©
        # x â†© W * x
        x â†© {
            elementwise â† ğ•© {ğ•¨.Mul ğ•©}Â¨ x
            total â† z {ğ•¨.Add ğ•©}Â´ elementwise
            total
        }Ë˜weights

        # x â†© x + b
        x â†© biases {
            ğ•¨.Add ğ•©
        }Â¨ x
        # x â†© sigmoid(x) = 1 Ã· (1 + exp(-x))
        x â†© { ((ğ•©.EExp@).Add 1).PowC Â¯1}Â¨ x
    }

    Objective â‡ {real_y ğ•Š y_hat:
        se â† real_y {(ğ•©.Sub ğ•¨).PowC 2}Â¨ y_hat
        z {ğ•¨.Add ğ•©}Â´ net.z {ğ•¨.Add ğ•©}Â´Ë˜ se
    }
    Update â‡ {ğ•Š loss:
        weights â†© (â†•â‰¢weights) {oâ€¿i ğ•Š w:
            idx â† (o Ã— insz) + i
            w.Sub lr Ã— idx âŠ‘ loss.eps
        }Â¨weights

        biases â†© (â†•â‰¢biases) {o ğ•Š w:
            idx â† (outsz Ã— insz) + o
            w.Sub lr Ã— idx âŠ‘ loss.eps
        }Â¨biases
    }
}

# # objective function:
# # f(x, y, z) = âŸ¨x > y, y < zâŸ©

# train_data â† âŸ¨
#     âŸ¨âŸ¨1, 2, 3âŸ©, âŸ¨0, 1âŸ©âŸ©,
#     âŸ¨âŸ¨5, Â¯1, 10âŸ©, âŸ¨1, 1âŸ©âŸ©,
#     âŸ¨âŸ¨2, 7, Â¯3âŸ©, âŸ¨0, 0âŸ©âŸ©,
#     âŸ¨âŸ¨20, 10, 0âŸ©, âŸ¨1, 0âŸ©âŸ©,
# âŸ©

# f(x, y, z) = âŸ¨~x | y, y | zâŸ©

train_data â† âŸ¨
    âŸ¨âŸ¨0, 0, 0âŸ©, âŸ¨1, 0âŸ©âŸ©,
    âŸ¨âŸ¨0, 0, 1âŸ©, âŸ¨1, 1âŸ©âŸ©,
    âŸ¨âŸ¨0, 1, 0âŸ©, âŸ¨1, 1âŸ©âŸ©,
    âŸ¨âŸ¨0, 1, 1âŸ©, âŸ¨1, 1âŸ©âŸ©,
    âŸ¨âŸ¨1, 0, 0âŸ©, âŸ¨0, 0âŸ©âŸ©,
    âŸ¨âŸ¨1, 0, 1âŸ©, âŸ¨0, 1âŸ©âŸ©,
    âŸ¨âŸ¨1, 1, 0âŸ©, âŸ¨1, 1âŸ©âŸ©,
    âŸ¨âŸ¨1, 1, 1âŸ©, âŸ¨1, 1âŸ©âŸ©,
âŸ©
train_x â† > {F xâ€¿y: x}Â¨ train_data
train_y â† > {F xâ€¿y: y}Â¨ train_data

net â† 3 Network 2

loss_history â† â†•0

Show â† {ğ•¤
    â€¢Out "x ="
    â€¢Show train_x

    y_hat â† {net.Forward ğ•©}Ë˜ train_x

    â€¢Out "y_hat ="
    â€¢Show {ğ•©.n}Â¨ y_hat
    â€¢Out "y ="
    â€¢Show train_y

    loss â† train_y net.Objective y_hat

    â€¢Out "loss = " âˆ¾ â€¢Fmt loss.n
}

Step â† {ğ•Š loud:
    y_hat â† {net.Forward ğ•©}Ë˜ train_x

    loss â† train_y net.Objective y_hat

    â€¢Out "loss = " âˆ¾ â€¢Fmt loss.n

    net.Update loss

    loss_history â†© loss_history âˆ¾ âŸ¨loss.nâŸ©
}

Show 1
StepâŸ1000 0
Show 1

# plot loss history

plt â† GnuPlot âŸ¨
    "title 'Neural network training'"
    "xlabel 'Epochs'"
    "ylabel 'Loss'"
âŸ©

plt.Plot loss_historyâ€¿"with lines"
plt.Show @
